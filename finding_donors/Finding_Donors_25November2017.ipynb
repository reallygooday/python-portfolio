{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning / Supervised Learning\n",
    "\n",
    "## Project: Finding Donors for CharityML\n",
    "\n",
    "Version: Python 2.7.14\n",
    "\n",
    "> Created: 25 November 2017\n",
    "> Meets Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Congratulations! Your revised submission is perfect, and you have done a great job to successfully complete this project on classification. Keep up your excellent work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Exploring the Data\n",
    "Student's implementation correctly calculates the following:\n",
    "Number of records\n",
    "Number of individuals with income >`$`50,000\n",
    "Number of individuals with income <=`$`50,000\n",
    "Percentage of individuals with income > `$`50,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done on getting the statistics. We can also examine if there are any missing values using pd.info(), and view a few samples using pd.head().\n",
    "\n",
    "From the data exploration, we notice that the number of income no greater than 50k ( n_at_most_50k) is more than three times the number of income greater than 50k (n_greater_50k). Therefore the data is unbalanced. Some techniques to handle unbalanced data include:\n",
    "\n",
    "Stratification, which preserves the relative portion of samples for each class\n",
    "Using precision / recall / F1 score as performance metric, rather than accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Preparing the Data\n",
    "Student correctly implements one-hot encoding for the feature and income data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Evaluating Model Performance\n",
    "Student correctly calculates the benchmark score of the naive predictor for both accuracy and F1 scores.\n",
    "Well done to get the scores. For the naive classifier, it is interesting to note that the precision is equivalent to accuracy, and recall remains at 1 consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pros and cons or application for each model is provided with reasonable justification why each model was chosen to be explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good choice of the models, and nice discussion.\n",
    "\n",
    "As you have noted, Decision Tree is popular in that it is easy to interpret, and can handle mixed type of features, both categorical and numerical (which is typical in most datasets nowadays). However, it tends to overfit easily. Other than managing overfitting by cross validation, we often use ensemble methods such as Random Forest or AdaBoost to improve the performance.\n",
    "\n",
    "Ensemble methods are very popular in data science competitions. There are two main categories of ensemble methods: bagging and boosting. Both build a strong model on a set of weak models. The difference is that bagging generates the weak learners with equal probability, whereas boosting tries to generate new models to target for cases where the previous models fail. Random Forest is a bagging technique, and examples of boosting include AdaBoost, and XGBoost. You can read more about their differences here: https://quantdare.com/what-is-the-difference-between-bagging-and-boosting.\n",
    "\n",
    "Lastly, Naive Bayes a simple algorithm, but it can be very powerful, as it is fast, and works well for small data. As Naive Bayes is built on probability, it is also robust to outliers / missing data. However, Naive Bayes assumes that every feature is independent of others. In practice, this assumption may not hold. So the performance of Naive Bayes is limited by the validity of the assumption. Violation of the assumption may degrade its performance.\n",
    "\n",
    "Udacity provides a good overview of most of the algorithms. After taking a couple of online courses, my personal preference is to get a deeper dive into the algorithms, and here are two of my favorite books (the former is introductory, and the latter is more involved):\n",
    "\n",
    "Introduction to Statistical Learning: http://www-bcf.usc.edu/~gareth/ISL/\n",
    "The Elements of statistical Learning: https://web.stanford.edu/~hastie/ElemStatLearn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Student successfully implements a pipeline in code that will train and predict on the supervised learning algorithm given.\n",
    "Great job to implement the classification pipeline. In addition to the F-score and computation time, we can also evaluate the models in more details using other sklearn functions, such as the confusion matrix and classification report. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#confusion matrix example\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print cm\n",
    "\n",
    "# classification report example\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "cr = classification_report(y_true, y_pred, target_names=target_names)\n",
    "print cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#example-model-selection-plot-confusion-matrix-py\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#classification-report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student correctly implements three supervised learning models and produces a performance visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Improving Results\n",
    "Justification is provided for which model appears to be the best to use given computational cost, model performance, and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is not the fastest, for this small data, we can focus more on the classification performance, and your choice of Random Forest is very reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student is able to clearly and concisely describe how the optimal model works in layman's terms to someone who is not familiar with machine learning nor has a technical background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice explanation of Random Forest. Here is another example that really helped me to understand Random Forest:\n",
    "\n",
    "http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model chosen is correctly tuned using grid search with at least one parameter using at least three settings. If the model does not need any parameter tuning it is explicitly stated with reasonable justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_fit = grid_obj.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noteworthy that here we are using the training set to tune the model, and keeping the test set away to avoid data leakage. As a general guideline, we should never touch the test set during model training and tuning, and should only use it for final model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student reports the accuracy and F1 score of the optimized, unoptimized, models correctly in the table provided. Student compares the final model results to previous results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not the case here, it is possible that the tuned model does not significantly improve over untuned one, or it could even give worse result than untuned one. The first checkpoint is whether the list of tuning parameters includes the default parameters. Sometimes the default parameters can yield the best performance.\n",
    "\n",
    "We may observe different results if we run the code with different random splits. This is largely due to the small and unbalanced dataset, and we can use techniques like StratifiedShuffleSplit to ensure that the ratio of the two classes are well maintained. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "...\n",
    "ssscv = StratifiedShuffleSplit( y_train, n_iter=10, test_size=0.1)\n",
    "grid = GridSearchCV(clf, parameters, cv = ssscv , scoring=f1_scorer) \n",
    "grid.fit( X_train, y_train ) \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Feature Importance\n",
    "Student ranks five features which they believe to be the most relevant for predicting an individual'sâ€™ income. Discussion is provided for why these features were chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student correctly implements a supervised learning model that makes use of the feature_importances_ attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, student discusses the differences or similarities between the features they considered relevant and the reported relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student analyzes the final model's performance when only the top 5 features are used and compares this performance to the optimized model from Question 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I absolutely agree. In general, a model with the full feature set should outperform the model with reduced feature set, as more features contribute more information. However, in real life projects, we often have to balance computation time against the model performance. If the data size is large, it is a good idea to select important features to simplify the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "## Predicting Boston Housing Prices / Project Review\n",
    "\n",
    "Created: 17 July 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You did an impressive job. I can see you worked really hard to improve your answers and you made it. The techniques and concepts we learned in this chapter are so important that a solid understanding of them builds the foundations to solve more complicated machine learning problems in the future.\n",
    "\n",
    "Last but not least, Andrew Ng made an excellent explanation on Deep Learning School on bias and variance tradeoff and you can also check his free book(https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/Machine_Learning_Yearning_V0.5_01.pdf) to learn more.\n",
    "\n",
    "Congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> All requested statistics for the Boston Housing dataset are accurately calculated. Student correctly leverages NumPy functionality to obtain these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You successfully calculated all the requested statistics from the dataset by implementing NumPy.\n",
    "\n",
    "Suggestions and Comments:\n",
    "\n",
    "You might want to leverage numpy’s functionality to obtain your results. For example,\n",
    "To obtain the number of houses and features in the dataset you could use the shape attribute of a numpy array, then index appropriately\n",
    "To obtain the minimum and maximum values of the houses you could use the np.amin and np.amax functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent on getting the statistics using numpy! It's always the first step to explore the dataset before implementing any machine learning algorithm.\n",
    "\n",
    "Some students are using pandas here but its default result of standard deviation is different from that of numpy. Here we need to calculate the std of the population, not samples. You can check their docs to see the difference.\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html\n",
    "http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.std.html?highlight=std#pandas.DataFrame.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Student correctly justifies how each feature correlates with an increase or decrease in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent reasoning and explanation. We can also check whether our analysis matches the machine learning algorithm result later in this project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You clearly understand the relationship between each parameter with the house price. Great job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Student correctly identifies whether the hypothetical model successfully captures the variation of the target variable based on the model’s R^2 score.\n",
    "The performance metric is correctly implemented in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, you successfully executed the R^2 score function and your discussion here is outstanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! The R^2 score is one of the metrics to evaluate model performance. We will learn many more in the coming lesson. For R^2 you also may want to check Khan's video and some of its caveats.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Coefficient_of_determination#Caveats\n",
    "\n",
    "Sklearn also has a dedicated page about all model evaluation methods.\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Student provides a valid reason for why a dataset is split into training and testing subsets for a model. Training and testing split is correctly implemented in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You gave great reasons for splitting a dataset, and nice implementation using sklearn's train_test_split!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Student correctly identifies the trend of both the training and testing curves from the graph as more training points are added. Discussion is made as to whether additional training points would benefit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good observation! One main difference between traditional ML algorithms (statistical learning) and deep learning is that the latter usually can make use of much more data (Big Data) to improve its performance. Traditional ML algorithms tend to become plateau after passing a certain size of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing job describing how the training and testing score change as the training set size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](curve.png \"ML Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Student correctly identifies whether the model at a max depth of 1 and a max depth of 10 suffer from either high bias or high variance, with justification using the complexity curves graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good understanding of bias and variance.\n",
    "\n",
    "In terms of overfitting and underfitting with different model complexity, sklearn also has a wonderful visualization to help us understand:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](plot_underfitting_overfitting.png \"Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job identifying that the model suffers from high bias when max_dept = 1 and that the model suffers from overfitting (high variance) when the max_dept = 10.\n",
    "\n",
    "Suggestions and Comments:\n",
    "\n",
    "Please check out this link\n",
    "(http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "if you want to understand more about model bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Student picks a best-guess optimal model with reasonable justification using the model complexity graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It is a good guess. You made this guess because of the explanation and understanding about overfitting and underfitting. It sounds reason. Let's see whether it matches the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Student correctly describes the grid search technique and how it can be applied to a learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a good understanding of what GridSearch is doing. The GridSearchCV in sklearn actually enumerates exhaustively every possible combination of given parameter options. It is also slow and memory inefficient. You can also check RandomizedSearchCV \n",
    "(http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) \n",
    "as an alternative. There's also an interesting paper about RandomizedSearch here\n",
    "(https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome explanation of the grid search algorithm, and your implementation was stellar :)\n",
    "\n",
    "Suggestions and Comments:\n",
    "\n",
    "If you haven't done so yet, you may check out the scikit-learn page for GridSearchCV\n",
    "(http://scikit-learn.org/stable/modules/grid_search.html).\n",
    "This page gives an excellent explanation of GridSearchCV, which can be useful in fully grasping concepts underlining GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Student correctly describes the k-fold cross-validation technique and discusses the benefits of its application when used with grid search when optimizing a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that the cv_results_ attribute in GridSearchCV\n",
    "(http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "magically self explained how GridSearchCV works. It only deals with training set (check fit_model(X_train, y_train)). First it will pick a combination of the hyperparameters, training on k-1 training set and validating on the kth validation set and then average the k validation scores to get a final score for this parameter combination. Then it repeats the process until all the combinations of parameters have a score and return the parameter set with the best score.\n",
    "\n",
    "You can print pd.DataFrame(grid.cv_results_) in your fit_model function to check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Student correctly implements the fit_model function in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect implementation of GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Student reports the optimal model and compares this model to the one they chose earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job here! Your max_depth matched exactly the best-guess optimal model max_depth you gave earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Student reports the predicted selling price for the three clients listed in the provided table. Discussion is made for each of the three predictions as to whether these prices are reasonable given the data and the earlier calculated descriptive statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You did a pretty good job in evaluating whether these prices are reasonable. Here we can compare their relative prices based on different features; compare their predicted prices with the real prices of houses with similar features and finally check whether their prices are within the range of max and min prices of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done predicting, these are valid prices for your clients’ houses. And also, your discussions on whether these prices are reasonable or not, sound great. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Student thoroughly discusses whether the model should or should not be used in a real-world setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When answering \"Is the model robust enough to make consistent predictions?\" you could also refer to the results of sensitivity above to support your conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice discussion here :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
